library(twitteR)
library(purrr)
library(tidytext)
library(dplyr)
library(tidyr)
library(lubridate)
library(scales)
library(broom)
library(ggplot2)
install.packages("purrr")
install.packages("purrr")
install.packages("tidytext")
install.packages("lubridate")
library(stringr)
library(twitteR)
library(purrr)
library(tidytext)
library(dplyr)
library(tidytext)
library(dplyr)
library(tidyr)
library(lubridate)
library(scales)
library(broom)
library(ggplot2)
library(scales)
library("scales", lib.loc="~/R/win-library/3.3")
consumerKey = 	'0ppSis0abzyBBEKq4Wle2mkQQ'
consumerSecret = 'XdFhYnYMyNPMe8SUtpojrCzqKIR5bg10x5t1HzQjMcCcZOL0CI'
accessToken = '146064028-H0771CDu3fhiOiF8xOvD0R3nwp03YzxC2kBr8fVc'
accessSecret = 'WStigiau31eiNXHriy9AC6cKttKPHxxo6pcAVSWJ6eOvW'
options(httr_oauth_cache=TRUE)
setup_twitter_oauth(consumer_key = consumerKey, consumer_secret = consumerSecret,
access_token = accessToken, access_secret = accessSecret)
setup_twitter_oauth(consumer_key = consumerKey, consumer_secret = consumerSecret,
access_token = accessToken, access_secret = accessSecret)
obamatweets<- userTimeline("potus44", n = 3200)
obamatweets_df <- tbl_df(map_df(obamatweets, as.data.frame))
View(obamatweets_df)
trdtweets<- userTimeline("tejas_rd", n = 100)
trdtweets_df <- tbl_df(map_df(trdtweets, as.data.frame))
View(trdtweets_df)
topwords <-
obamatweets_df %>%
paste(collapse = " ") %>%
str_split("\\s") %>%
unlist %>%
tolower %>%
removePunctuation %>%
removeWords(stopwords("english")) %>%
#wordStem %>%
.[. != ""] %>%
table %>%
sort(decreasing = TRUE) %>%
head(100)
install.packages("wordcloud")
install.packages("tm")
library(twitteR)
library(SnowballC)
library(wordcloud)
library(tm)
library(tm)
library(stringr)
library(dplyr)
library(wordcloud)
topwords <-
obamatweets_df %>%
paste(collapse = " ") %>%
str_split("\\s") %>%
unlist %>%
tolower %>%
removePunctuation %>%
removeWords(stopwords("english")) %>%
#wordStem %>%
.[. != ""] %>%
table %>%
sort(decreasing = TRUE) %>%
head(100)
wordcloud(names(topwordscloud), topwords, min.freq = 3)
library(wordcloud)
wordcloud(names(topwordscloud), topwords, min.freq = 3)
wordcloud(names(topwordcloud), topwords, min.freq = 3)
wordcloud(names(topwords), topwords, min.freq = 3)
wordcloud(names(topwords), topwords, min.freq = 3)
trdtweets<- userTimeline("realDONALDTRUMP", n = 1000)
trumptweets<- userTimeline("realDONALDTRUMP", n = 1000)
trumptweets_df <- tbl_df(map_df(trumptweets, as.data.frame))
topwords <-
trumptweets_df %>%
paste(collapse = " ") %>%
str_split("\\s") %>%
unlist %>%
tolower %>%
removePunctuation %>%
removeWords(stopwords("english")) %>%
#wordStem %>%
.[. != ""] %>%
table %>%
sort(decreasing = TRUE) %>%
head(1000)
wordcloud(names(topwords), topwords, min.freq = 3)
View(trumptweets_df)
united_tweets <- searchTwitter('united', n = 10000)
united_tweets <- searchTwitter('united', n = 10)
united_tweets
united_tweets <- searchTwitter('united airline', n = 10)
united_tweets
united_tweets <- searchTwitter('united airline', n = 1000)
data.frame(united_tweets)
source('~/08 Udemy/04 Web scrapping/twitter_scrapping.R', echo=TRUE)
as.data.frame(united_tweets)
head(united_tweets)
str(united_tweets)
stock_list <- c("UAL", "AAPL")
stock_list <- c("UAL", "AAPL")
install.packages('QuandlR')
install.packages("Quandl")
library(Quandl)
trial <- https://www.quandl.com/api/v3/datasets/EOD/AAPL.csv?api_key=bqS1Phqz9PssFmrV4G5s
trial <- 'https://www.quandl.com/api/v3/datasets/EOD/AAPL.csv?api_key=bqS1Phqz9PssFmrV4G5s'
webp <- read.csv(trial)
View(webp)
plot(webp$Close)
ggplot(webp, aes(Date, Adj_Close)) + geom_line() +
scale_x_date(format = "%b-%Y") + xlab("Price") + ylab("Daily Views")
library(ggplot2)
ggplot(webp, aes(Date, Adj_Close)) + geom_line() +
scale_x_date(format = "%b-%Y") + xlab("Price") + ylab("Daily Views")
ggplot(webp, aes(Date, Adj_Close)) +
geom_line() +
xlab("Price") + ylab("Daily Views")
Y
ggplot(webp, aes(Date, Adj_Close)) +
geom_line()
ggplot(webp, aes(Date, Adj_Close)) + geom_line()
ggplot(webp, aes(Date, Adj_Close)) + geom_line()
ggplot(webp, aes(Date, Adj_Close)) + geom_point()
install.packages('swirl')
library('swirl')
rm(list=ls())
swirl()
setwd("~/08 Udemy/02 Bomu the machine learner/Machine Learning A-Z Template Folder/Part 3 - Classification/Section 16 - Support Vector Machine (SVM)")
#load data
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
library(e1071)
classifier = svm(formula = Purchased ~ .,
data = training_set,
type = 'C-classification',
kernel = 'polynomial')
y_pred = predict(classifier, newdata = test_set[-3])
cm = table(test_set[,3], y_pred)
cm
library(ElemStatLearn)
set = training_set
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
View(dataset)
View(test_set)
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'SVM (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(ROCR)
pred = prediction(test_set[3], y_pred)
setwd("~/08 Udemy/02 Bomu the machine learner/Machine Learning A-Z Template Folder/Part 7 - Natural Language Processing/Section 36 - Natural Language Processing")
#always better to have in tsv format for NLP
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE) #by default sep is tab. Also we are asking it to ignore any quotes in the parameters
#stringasfactors parameters prevents from identifying reviews column as factors
#now text cleaning step
install.packages('tm') #text mining package
library(tm)
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower)) #making all text low caps
#use as.character(corpus[[1]]) to see one entry
corpus = tm_map(corpus, removeNumbers) #to remove all numbers in reviews
corpus = tm_map(corpus, removePunctuation) #remove all punctuations
#install.packages('SnowballC') stopwords() belongs to this
library(SnowballC)
corpus = tm_map(corpus, removeWords, stopwords()) #remove useless words. You can use built-in stock words list
corpus = tm_map(corpus, stemDocument) #to do steming, remove tenses and focus on roots ony
corpus = tm_map(corpus, stripWhitespace)#remove extra spaces
#creating the bag of words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999) #filter non frequent words. Keep 99% of the words
#these words are non frequent and hence not relevant
#Converting from matrix to dataframe
#just use the RF template. Create the dataframe here. But use the other code there.
#remeber to convert matrix into dataframe though. RF and other classification codes work on dataframes
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
#************************Building classifier now**********************************
# Encoding the target feature as factor
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling - nt needed because its 0s and 1s
#training_set[-3] = scale(training_set[-3])
#test_set[-3] = scale(test_set[-3])
#create classifier
#install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 10)
#be careful while choosing ntree since higher might be good but lead to overfitting
#overfitting will lead to some poor predictions
y_pred = predict(classifier,newdata = test_set[-692])
# Making the Confusion Matrix
cm = table(test_set[, 692], y_pred)
install.packages("caTools")
cm = table(test_set[, 692], y_pred)
cm
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 11)
y_pred = predict(classifier,newdata = test_set[-692])
cm = table(test_set[, 692], y_pred)
cm
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 8)
cm = table(test_set[, 692], y_pred)
cm
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 8)
y_pred = predict(classifier,newdata = test_set[-692])
cm = table(test_set[, 692], y_pred)
cm
library(wordcloud)
library(dplyr)
library(stringr)
library(stringr)
library(dplyr)
View(dataset_original)
topwords <-
dataset_original %>%
paste(collapse = " ") %>%
str_split("\\s") %>%
unlist %>%
tolower %>%
removePunctuation %>%
removeWords(stopwords("english")) %>%
#wordStem %>%
.[. != ""] %>%
table %>%
sort(decreasing = TRUE) %>%
head(1000)
wordcloud(names(topwords), topwords, min.freq = 3)
topwords <-
dataset_original[1] %>%
paste(collapse = " ") %>%
str_split("\\s") %>%
unlist %>%
tolower %>%
removePunctuation %>%
removeWords(stopwords("english")) %>%
#wordStem %>%
.[. != ""] %>%
table %>%
sort(decreasing = TRUE) %>%
head(1000)
wordcloud(names(topwords), topwords, min.freq = 3)
View(dataset)
summary(classifier)
cm
setwd("~/08 Udemy/02 Bomu the machine learner/Machine Learning A-Z Template Folder/Part 2 - Regression/Section 5 - Multiple Linear Regression")
dataset = read.csv('50_Startups.csv')
dataset$State = factor(dataset$State,
levels = c('New York', 'California', 'Florida'),
labels = c(1,2,3)) #c is a vector
library(caTools)
set.seed(123456) #random number seed made to make split
split = sample.split(dataset$Profit, SplitRatio = 0.8) #split is for training set
training_set = subset(dataset,split == TRUE) #finally assigning
test_set = subset(dataset,split == FALSE) #finally assigning
regressor = svm(formula = Profit ~ .,
data = training_set,
type = 'eps-regression',
kernel = 'polynomial')
summary(regressor)
y_pred = predict(regressor, newdata = test_set)
y_pred
head(y_pred)
test_set
table(y_pred, test_set$Profit)
table(y_pred, test_set$Profit)
dfm <- data.frame(y_pred)
View(dfm)
dfm$Old <- test_set$Profit
summary(regressor)
dfm[2]-dfm[1]
regressor = lm(formula = Profit ~ .,
data = training_set)
y_pred = predict(regressor, newdata = test_set)
summary(regressor)
ggplot2(training_set$Profit, aes(xvar,yvar))
library(ggplot2)
ggplot2(training_set$Profit, aes(xvar,yvar))
ggplot(training_set$Profit, aes(xvar,yvar))
View(dataset)
y_pred1 = predict(classifier,newdata = test_set[-692], type = "response")
y_pred = predict(regressor, newdata = test_set, type = "response")
y_pred
library(pROC)
install.packages("pROC")
library(pROC)
plot(roc(test_set$Profit, y_pred, direction="<"),
col="yellow", lwd=3, main="The turtle finds its way")
setwd("~/08 Udemy/02 Bomu the machine learner/Machine Learning A-Z Template Folder/Part 7 - Natural Language Processing/Section 36 - Natural Language Processing")
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE) #by default sep is tab. Also we are asking it to ignore any quotes in the parameters
library(tm)
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower)) #making all text low caps
corpus = tm_map(corpus, removeNumbers) #to remove all numbers in reviews
corpus = tm_map(corpus, removePunctuation) #remove all punctuations
library(SnowballC)
corpus = tm_map(corpus, removeWords, stopwords()) #remove useless words. You can use built-in stock words list
corpus = tm_map(corpus, stemDocument) #to do steming, remove tenses and focus on roots ony
corpus = tm_map(corpus, stripWhitespace)#remove extra spaces
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999) #filter non frequent words. Keep 99% of the words
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 10)
y_pred = predict(classifier,newdata = test_set[-692])
cm = table(test_set[, 692], y_pred)
library(pROC)
plot(roc(test_set[692], y_pred, direction="<"),
col="yellow", lwd=3, main="ROC for reviews")
y_pred = predict(classifier,newdata = test_set[-692], type = "response")
plot(roc(test_set[692], y_pred, direction="<"),
col="yellow", lwd=3, main="ROC for reviews")
plot(roc(test_set[692], unlist(y_pred), direction="<"),
col="yellow", lwd=3, main="ROC for reviews")
plot(roc(unlist(test_set[692]), unlist(y_pred), direction="<"),
col="yellow", lwd=3, main="ROC for reviews")
dataset = read.csv('GermanCredit')
dataset = read.csv('GermanCredit.csv')
setwd("~/08 Udemy/02 Bomu the machine learner/Credit risk modeling")
dataset = read.csv('GermanCredit.csv')
dataset = read.csv('GermanCredit.csv')
str(dataset)
GermanCredit <- dataset
GermanCredit$Age_c1 <- ifelse(GermanCredit$Age <=26,1,0)
GermanCredit$Age_c2 <- ifelse(GermanCredit$Age >26 & GermanCredit$Age <=33,1,0)
GermanCredit$Amount_c1 <- ifelse(GermanCredit$Amount <=1260,1,0)
GermanCredit$Amount_c2 <- ifelse(GermanCredit$Amount >1260 & GermanCredit$Amount <=4700,1,0)
GermanCredit$Duration_c1 <- ifelse(GermanCredit$Duration <=15,1,0)
GermanCredit$Duration_c2 <- ifelse(GermanCredit$Duration >15 & GermanCredit$Duration <=30,1,0)
library(caTools)
dataset = read.csv('GermanCredit.csv')
str(dataset)
dataset <- dataset
dataset$Age_c1 <- ifelse(dataset$Age <=26,1,0)
dataset$Age_c2 <- ifelse(dataset$Age >26 & dataset$Age <=33,1,0)
dataset$Amount_c1 <- ifelse(dataset$Amount <=1260,1,0)
dataset$Amount_c2 <- ifelse(dataset$Amount >1260 & dataset$Amount <=4700,1,0)
dataset$Duration_c1 <- ifelse(dataset$Duration <=15,1,0)
dataset$Duration_c2 <- ifelse(dataset$Duration >15 & dataset$Duration <=30,1,0)\
dataset$Duration_c2 <- ifelse(dataset$Duration >15 & dataset$Duration <=30,1,0)
library(caTools)
set.seed(123456) #random number seed made to make split
View(dataset)
split = sample.split(dataset$Class, SplitRatio = 0.8) #split is for training set
training_set = subset(dataset,split == TRUE) #finally assigning
test_set = subset(dataset,split == FALSE) #finally assigning
classifier <- glm(formula = Class ~ ., family = binomial, data = training_set)
summary(classifier)
install.packages("car")
library(car)
vif(classifier)
step(classifier, direction = "both")
vif(classifier)
classifier <- glm(Class ~
#Amount +
#InstallmentRatePercentage +
#ForeignWorker +
CheckingAccountStatus.lt.0 +
CheckingAccountStatus.0.to.200 +
CreditHistory.NoCredit.AllPaid +
CreditHistory.ThisBank.AllPaid +
CreditHistory.PaidDuly +
#CreditHistory.Delay +
Purpose.NewCar +
#Purpose.Furniture.Equipment +
#Purpose.Radio.Television +
#Purpose.Repairs +
Purpose.Education +
#Purpose.Business +
SavingsAccountBonds.lt.100 +
#SavingsAccountBonds.100.to.500 +
EmploymentDuration.4.to.7 +
#Personal.Male.Single +
OtherDebtorsGuarantors.None +
OtherDebtorsGuarantors.CoApplicant +
#Property.RealEstate +
#OtherInstallmentPlans.Bank +
#Housing.Rent +
Age_c1 +
#Amount_c1 +
Duration_c1 + Duration_c2,
family = binomial,
data = training_set)
summary(classifier)
y_pred = predict(classifier,newdata = test_set[-10]
l
y_pred = predict(classifier,newdata = test_set[-10])
cm = table(test_set[, 10], y_pred)
cm
View(training_set)
View(test_set)
prob_pred = predict(classifier, type = 'response', newdata = test_set[-10])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
cm = table(test_set[,10], y_pred)
cm
cm = table(test_set[,10], y_pred, row.names("actual"))
cm = table(test_set[,10], y_pred, row.names(actual))
cm = table(test_set[,10], y_pred)
summary(classifier)
library(e1071)
library(caret)
confusionMatrix(data=factor(test_set[,10]),reference = y_pred,positive = '1')
library(ROCR)
perf.obj <- prediction(predictions=prob_pred,
labels=test_set$Class)
perf.obj
roc.obj <- performance(perf.obj, measure="tpr", x.measure="fpr")
plot(roc.obj,
main="German Credit - ROC Curves",
xlab="1 - Specificity: False Positive Rate",
ylab="Sensitivity: True Positive Rate",
col="blue")
abline(0,1,col="grey")
plot(roc.obj,
main="Credit Risk model - ROC",
xlab="False Positive Rate",
ylab="True Positive Rate",
col="blue")
abline(0,1,col="grey")
library(ggplot2)
ggplot(roc.obj, aes("False", "True"))
ggplot(roc.obj, aes("False", "True")) + geom_line()
abline(0,1,col="red")
abline(0,1,col="grey")
summary(classifier)
head(dataset)
summary(dataset)
int(dataset)
str(dataset)
str(dataset)
cm
Accuracy = cm(1,1) + cm(2,2)/(cm(1,1) +cm(1,2) + cm(2,1) + cm(2,2))
Accuracy = (cm(1,1) + cm(2,2))/(cm(1,1) +cm(1,2) + cm(2,1) + cm(2,2))
cm(1,1)
cm[1,1]
Accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] +cm[1,2] + cm[2,1] + cm[2,2])
Accuracy
Accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] +cm[1,2] + cm[2,1] + cm[2,2])*100%
Accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] +cm[1,2] + cm[2,1] + cm[2,2])*100
cm[1,2]
Precision = (cm[2,2])/(cm[2,2] +cm[1,2])*100
Precision
Recall = (cm[2,2])/(cm[2,2] +cm[2,1])*100
Accuracy
Precision
Recall
